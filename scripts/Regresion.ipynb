{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b50214",
   "metadata": {},
   "source": [
    "# Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fd294",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecfcb6",
   "metadata": {},
   "source": [
    "Un modelo de regresión lineal predice la respuesta como la suma ponderada de features. La linealidad de la relación aprendida hace que la interpretación sea simple.\n",
    "\n",
    "Ls modelos lineales pueden utilizarse para modelar la dependencia de una variable respuesta **y** versus un conjunto de features **x**. La relación aprendida puede escribirse como:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 X_1 +\\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ba46f",
   "metadata": {},
   "source": [
    "El valor predicho para una instancia es la suma ponderada de sus p features.  Los $\\beta_i$ representan los pesos de los features. El primero es $\\beta_0$ que se conoce como intercepto y no se multiplica por ningún feature, por otro lado $\\epsilon$ es el error que aún se comete, esto es, la diferencia entre la perdicción y el verdadero valor. Estos errores se asumen con una distribución normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd32b6",
   "metadata": {},
   "source": [
    "Para encontrar el mejor coeficiente, se suele minimizar el cuadrado de la diferencia entre los valores estimados y los valores actuales. \n",
    "La principal ventaja de los modelos de regresión es la linealidad, hace que el proceso de estimación sea simple y lo más importante es que estas ecuaciones tienen una interpretación muy fácil de entender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc0ea4",
   "metadata": {},
   "source": [
    "Los pesos estimados se acompañan con intervalos de confianza, en el cual se estima que el peso cubre al verdadero valor con una cierta confianza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5203b0c",
   "metadata": {},
   "source": [
    "Si el modelo es *\"correcto\"* depende si las relaciones en los datos cumplen ciertos supuestos que son:\n",
    "\n",
    "- **Linealidad**: los modelos de regresión son una combinación lineal de variables, lo cual es una gran fortaleza y una grande limitación. La linealidad resulta en modelos interpretables que son fáciles de cuantificar y describir. Son aditivos, lo que implica que es posible aislar efectos.  Si se sospecha interacción de variables o asociación no lineal, pueden incluirse términos de interacción o splines.\n",
    "\n",
    "- **Normalidad**: se asume que la variable respuesta sigue una distribución normal, si este supuesto no se cumple, los intervalos de confianza estimados son inválidos.\n",
    "\n",
    "- **Homocedasticidad**:  se asume que la variancia del error es constante en el tiempo. Este es un supuesto que no se suele cumplir en la realidad.\n",
    "\n",
    "- **Independencia**: se asume que cada instancia es independiente de otra instancia. \n",
    "\n",
    "- **Efectos Fijos**: los features son considerados fijos, esto quiere decir que son tratados como constantes y no como variables aleatorias. Esto implica que son libres de erroes de medición, lo cual es otro supuesto poco realista. Sin embargo, sin este supuesto, debería ajustarse modelos mucho más complejos.\n",
    "\n",
    "- **Ausencia de multicolinealidad**: no se desea que haya variables altamente correlacionadas porque esto arruina los pesos de los features, hace que sea muy difícil estimar los pesos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe347ca",
   "metadata": {},
   "source": [
    "## Interpretación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11999c74",
   "metadata": {},
   "source": [
    "La interpretación del modelo de regresión va a depender del tipo de variable:\n",
    "\n",
    "- **Feature numérico**: ante cambios unitarios de la variable, aumenta la respuesta en el peso estimado para dicha variable.\n",
    "\n",
    "- **Feature binario** : es una variable que toma dos posibles valores (0: ausencia del atributo - 1: presencia del atributo). El coeficiente indica cuánto cambia la respuesta al pasar del valor de referencia (0) al otro valor (1).\n",
    "\n",
    "- **Variable con N categorías**: una variable con un número fijo de valores posibles (A-B-C-D). Una solución es usar *codificación one-hot*, que implica que cada categoría tiene su propia columna binaria. Para variables categóricas con D categorías, se necesitan D-1 columnas. La interpretación para cada categoría es similar al caso binario, se mide el cambio respecto a la variable de referencia.\n",
    "\n",
    "- **Intercepto**: el intercepto es un feature constante, como si fuera que la variable que acompaña a ese coeficiente vale 1. Se interpreta como, cuando todos los feature valen cero y todas las variables categóricas se encuentran en su valor de referencia, la predicción del modelo es el peso del intercepto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fcf6c",
   "metadata": {},
   "source": [
    "- **Feature numérico**: el aumento del feature $x_j$ en una unidad aumenta/disminuye la predicción de $y$ en $\\beta_j$ unidades cuando las demás variables permanecen constantes.\n",
    "\n",
    "- **Feature categórcio**:  pasar de la categoría de referencia de $x_j$ a otra categoría, aumenta/disminuye la predicción de  $y$ en $\\beta_j$ unidades cuando las demás variables permanecen constantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dd659",
   "metadata": {},
   "source": [
    "Otra métrica importante paara interpretar los modelos de regresión es el $R^2$, mientras más alto mejor explica el modelo los datos. El tema es que siempre aumenta al aumentar el número de features, es por esto que es mejor usar $R^2_{ajustado}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75852cf",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711471d",
   "metadata": {},
   "source": [
    "La importancia de una variable en un modelo de regresión lineal puede ser medido como el valor absoluto de la **estadística t**. \n",
    "\n",
    "$$t_{\\hat{\\beta_j}} = \\frac{\\hat{\\beta_j}}{SE(\\hat{\\beta_j})}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f20c3",
   "metadata": {},
   "source": [
    "La importancia de una variable aumenta al aumentar su coeficiente, mientras mayor variancia tenga el peso estimado (menos seguridad sobre el valor correcto), menos importante debe ser el feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84137cc0",
   "metadata": {},
   "source": [
    "## Aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615eb9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51132208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93193bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
